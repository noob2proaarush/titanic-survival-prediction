# -*- coding: utf-8 -*-
"""Titanic Survival Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SNttLwtQgbpCW-pSVg9_lw3663IvgT2A
"""

!pip install seaborn
import seaborn as sns
import pandas as pd

# Load the Titanic dataset
df = sns.load_dataset('titanic')

# 🧹 Step 2: Data Cleaning & Preprocessing

# 2.1 Drop 'Cabin' column
try:
    df.drop('Cabin', axis=1, inplace=True)
except KeyError:
    print("Cabin column not found or already dropped.")
    # If the 'Cabin' column isn't present, the code continues without errors

# 2.2 Fill missing 'Age' with the median
df['Age'].fillna(df['Age'].median(), inplace=True)

# 2.3 Fill missing 'Embarked' with the mode
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# 2.4 Convert 'Sex' to numerical values
df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})

# 2.5 One-hot encode 'Embarked' column (drop first to avoid dummy variable trap)
df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)

# 2.6 Final check
df.info()
df.head()

df

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Improved and Optimized Titanic Classification Pipeline

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load dataset and drop less useful columns
df = sns.load_dataset('titanic')
df.drop(['deck', 'embark_town', 'alive', 'class', 'who', 'adult_male'], axis=1, inplace=True)

# 1. Class Distribution
print("\n1. Class Distribution (Before SMOTE):")
y = df['survived']
y.value_counts().plot(kind='bar', title='Class Distribution')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.show()

# 2. Mapping True/False
print("\n2. True and False Counts:")
print(y.map({0: 'False', 1: 'True'}).value_counts())

# 3. Null Value Check
print("\n3. Null Values in Dataset:")
print(df.isnull().sum())

# Helper function to preprocess and train model
def train_model(df, strategy):
    X = df.drop('survived', axis=1)
    y = df['survived']

    num_cols = X.select_dtypes(include=['int64', 'float64']).columns
    cat_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns

    numeric_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    categorical_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))
    ])

    preprocessor = ColumnTransformer([
        ('num', numeric_pipeline, num_cols),
        ('cat', categorical_pipeline, cat_cols)
    ])

    model_pipeline = Pipeline([
        ('preprocessing', preprocessor),
        ('classifier', RandomForestClassifier(random_state=42))
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)
    model_pipeline.fit(X_train, y_train)
    y_pred = model_pipeline.predict(X_test)

    print(f"\n4. Accuracy ({strategy} Nulls):", accuracy_score(y_test, y_pred))
    print(f"\n4. Classification Report ({strategy} Nulls):")
    print(classification_report(y_test, y_pred))
    return model_pipeline, X, y

# 4. Removing Null Values
df_dropna = df.dropna()
model_pipeline_dropna, X_dropna, y_dropna = train_model(df_dropna, 'Dropped')

# 5. Replacing Null Values
df_filled = df.copy()
for col in df_filled.select_dtypes(include=['int64', 'float64']):
    df_filled[col].fillna(df_filled[col].median(), inplace=True)
for col in df_filled.select_dtypes(include=['object', 'category', 'bool']):
    df_filled[col].fillna(df_filled[col].mode()[0], inplace=True)

model_pipeline_filled, X_filled, y_filled = train_model(df_filled, 'Filled')

# 7. Train-Test-Validation Split using Filled Data
X_train_val, X_test, y_train_val, y_test = train_test_split(X_filled, y_filled, test_size=0.2, stratify=y_filled, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)

# Train again on train set and validate
model_pipeline_filled.fit(X_train, y_train)
y_val_pred = model_pipeline_filled.predict(X_val)
print("\n8. Validation Set Results:")
print(classification_report(y_val, y_val_pred))

# Final test evaluation
y_test_pred = model_pipeline_filled.predict(X_test)
print("\n8. Final Test Set Results:")
print(classification_report(y_test, y_test_pred))

# Install required library in Colab (run this only once)
# !pip install imblearn

# Imports
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Load dataset
df = sns.load_dataset('titanic')
df.drop(['deck', 'embark_town', 'alive', 'class', 'who', 'adult_male'], axis=1, inplace=True)

# Null values check
print("🔍 Null Values:\n", df.isnull().sum())

# Feature/Target split
X = df.drop('survived', axis=1)
y = df['survived']

# Class distribution before SMOTE
print("\n Class Distribution Before SMOTE:")
print(y.value_counts())
sns.countplot(x=y)
plt.title("Class Distribution (Before SMOTE)")
plt.show()

# Identify column types
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns

# Preprocessing Pipelines
numeric_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', numeric_pipeline, numerical_cols),
    ('cat', categorical_pipeline, categorical_cols)
])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Pipeline WITHOUT SMOTE
pipeline_no_smote = Pipeline([
    ('preprocessing', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

pipeline_no_smote.fit(X_train, y_train)
y_pred_no_smote = pipeline_no_smote.predict(X_test)

print("\n WITHOUT SMOTE:")
print("Accuracy:", accuracy_score(y_test, y_pred_no_smote))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_no_smote))
print("Classification Report:\n", classification_report(y_test, y_pred_no_smote))

# Pipeline WITH SMOTE
pipeline_smote = ImbPipeline([
    ('preprocessing', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier', RandomForestClassifier(random_state=42))
])

pipeline_smote.fit(X_train, y_train)
y_pred_smote = pipeline_smote.predict(X_test)

print("\n WITH SMOTE:")
print("Accuracy:", accuracy_score(y_test, y_pred_smote))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_smote))
print("Classification Report:\n", classification_report(y_test, y_pred_smote))

''' To improve the **readability and appearance** of your report in **Google Colab**, it's best to use **Markdown formatting with proper spacing**, **LaTeX for math**, and **clear visual structure** (like bullet points, headers, and horizontal lines).

Here's a cleaned-up and visually enhanced version that you can paste directly into a Colab text cell:

---

## 🔍 **Model Comparison: Precision With and Without SMOTE**

---

### 🎯 **Objective**

The goal of this analysis is to evaluate the impact of **SMOTE (Synthetic Minority Over-sampling Technique)** on the performance of a **Random Forest Classifier** using the Titanic dataset. The focus is on **precision**, particularly for the minority class (*Survived = 1*).

---

### 📘 **Definitions**

**Precision** is a key evaluation metric for classification problems, especially when the dataset is imbalanced. It tells us how many of the passengers **predicted as survived** actually survived.

#### 📐 **Precision Formula**

\[
\text{Precision} = \frac{TP}{TP + FP}
\]

Where:

- \( TP \) = True Positives
- \( FP \) = False Positives

> In this context:
> - **True Positives**: Passengers correctly predicted as survived.
> - **False Positives**: Passengers incorrectly predicted as survived.

---

### **Approach**

We implemented and compared two models:

1. **Model with SMOTE**
   - SMOTE is used to balance the class distribution by oversampling the minority class (*Survived = 1*).
   - Pipeline: **Preprocessing** → **SMOTE** → **Random Forest Classifier**

2. **Model without SMOTE**
   - The model is trained on the original imbalanced dataset.
   - Pipeline: **Preprocessing** → **Random Forest Classifier**

---

### **Evaluation Metrics**

Both models were evaluated using:

-  **Confusion Matrix**
-  **Classification Report**:
  - Precision
  - Recall
  - F1-score
  - Support

---

### **Results Summary**

| Metric                    | Model with SMOTE | Model without SMOTE |
|--------------------------|------------------|---------------------|
| **Precision (Survived)** | [Insert Value]   | [Insert Value]      |
| **Recall (Survived)**    | [Insert Value]   | [Insert Value]      |
| **F1-Score (Survived)**  | [Insert Value]   | [Insert Value]      |

---

## **Analysis**

- **SMOTE Impact**: By balancing the dataset, SMOTE gives the model more exposure to the minority class, often improving recall and possibly precision.
- **Precision Insight**:
  - If precision is **higher** with SMOTE → the model is better at predicting survivors accurately.
  - If precision is **lower** with SMOTE → it may indicate **overfitting** or that synthetic examples introduced **noise**.

---

### **Conclusion**

- If SMOTE improves precision, it shows value in tackling class imbalance.
- If it doesn’t, the model may already handle the imbalance well without oversampling.

> **Takeaway**: SMOTE can be powerful in imbalanced scenarios but should be evaluated carefully for each use case.

---

Let me know if you’d like this formatted with **collapsible sections**, **interactive charts**, or if you want to include **code + output** together in the final notebook version.'''